{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the State Farm image data\n",
    "\n",
    "This notebook provides analysis of the provided State Farm data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "First, let's import what we need and set up environment variables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports of the relevant libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import caffe\n",
    "import lmdb\n",
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# These are the locations of the images provided by Kaggle\n",
    "# Root Dir is needed for Python, but not for create lmdb shell script later... (we need it there too!)\n",
    "image_root_dir = './imgs/'\n",
    "train_image_source_dir = \"./train\"\n",
    "test_image_source_dir = \"./test\"\n",
    "driver_image_list = \"./driver_imgs_list.csv\"\n",
    "\n",
    "# These are the locations of the images that we will work with \n",
    "# Note that as we're continually mix up training and validation drivers/images, \n",
    "# then we will store images in one directory and use code to determine whether to train or validate\n",
    "train_images = \"./images/train\"\n",
    "valid_images = \"./images/validate\" \n",
    "test_images = \"./images/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the list of training images\n",
    "There are only 27 different drivers so in order to avoid overfitting, or testing using very similar data to training, we will split the data based on the driver into train and validation sets.\n",
    "\n",
    "Initially though, let's get the list of drivers, see how many images are available for each driver, and which classification they have been labelled with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject classname            img\n",
      "0    p002        c0  img_44733.jpg\n",
      "1    p002        c0  img_72999.jpg\n",
      "2    p002        c0  img_25094.jpg\n",
      "3    p002        c0  img_69092.jpg\n",
      "4    p002        c0  img_92629.jpg\n"
     ]
    }
   ],
   "source": [
    "driver_list = pd.read_csv(conf.driver_image_list)\n",
    "print driver_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the drivers into a training and validation set.  To ensure we don't have overfitting (the training set and the validation set contain the same or similar images) we will split on drivers, so a driver can only appear in training or validation but not both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following drivers: ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081']\n"
     ]
    }
   ],
   "source": [
    "driver_ids = []\n",
    "for id, driver in driver_list.iterrows():\n",
    "    if driver['subject'] not in driver_ids:\n",
    "        driver_ids.append(driver['subject'])\n",
    "print \"Found the following drivers: {}\".format(driver_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and validation data tests (split = percentage to have in training set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver train list: ['p042' 'p064' 'p056' 'p049' 'p050' 'p015' 'p039' 'p045' 'p016' 'p066'\n",
      " 'p051' 'p041' 'p014' 'p026' 'p081' 'p047' 'p021' 'p035' 'p061']\n",
      "Driver validation list: ['p002', 'p012', 'p022', 'p024', 'p052', 'p072', 'p075']\n"
     ]
    }
   ],
   "source": [
    "def split_drivers_into_train_and_validate(driver_list, split = 0.75):\n",
    "    driver_valid_list = []\n",
    "    # Take a random sample of drivers into the training list\n",
    "    driver_train_list = np.random.choice(driver_list, int(len(driver_list)*split), replace = False)\n",
    "    # Take the remaining drivers into the validation list\n",
    "    driver_valid_list = [ driver for driver in driver_list if driver not in driver_train_list]\n",
    "    return driver_train_list, driver_valid_list\n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_all.ix[rows], student_data[target_col].ix[rows], test_size=test_size)\n",
    "\n",
    "training_list, validation_list = split_drivers_into_train_and_validate(driver_ids)\n",
    "print \"Driver train list: {}\".format(training_list)\n",
    "print \"Driver validation list: {}\".format(validation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of the drivers, their classification and their images, let's loads images one by one\n",
    "and get the dimensions of the image (put this into the dataframe too!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_image_data(driver_list, filter, render = True):\n",
    "    \"\"\"Split the data based on a filter (train, valid, test, etc.).  \n",
    "    render is used if we want to see the first image only as a sample of the data\"\"\"\n",
    "    image_list = []\n",
    "    class_list = []\n",
    "    class_count = dict()\n",
    "    driver_count = dict()\n",
    "    total = 0\n",
    "    # for idx, driver in driver_list.iterrows():\n",
    "    for driver_row in [ drvr for drvr in driver_list.iterrows() if drvr[1]['subject'] in filter ]:\n",
    "        driver = driver_row[1]  # Drop the index created by the Pandas Dataframe\n",
    "        img_filename = train_image_source_dir + '/' + driver['classname'] + '/' + driver['img']\n",
    "        driver_class = int(driver['classname'][1:])  # Get integer to represent class\n",
    "        image_list.append(img_filename)\n",
    "        class_list.append(driver_class)\n",
    "        # Display the first image if render is set to True (by default we do this!)\n",
    "        if render == True:\n",
    "            image = caffe.io.load_image(image_root_dir + img_filename)            \n",
    "            #image = caffe.io.resize_image(image, (120, 160), interp_order = 3 )\n",
    "            print image.shape\n",
    "            plt.imshow(image)\n",
    "            render = False\n",
    "        # Track how many images we get for each class\n",
    "        if driver_class in class_count:\n",
    "            class_count[driver_class] += 1\n",
    "        else:\n",
    "            class_count[driver_class] = 1\n",
    "        # Track the number of drivers\n",
    "        if driver['subject'] in driver_count:\n",
    "            driver_count[driver['subject']] += 1\n",
    "        else:\n",
    "            driver_count[driver['subject']] = 1\n",
    "        # Keep a running total and track progress\n",
    "        total += 1\n",
    "        if total % 100 == 0:\n",
    "            print \".\",\n",
    "    # Print some useful stats   \n",
    "    print \n",
    "    #print \"Found the following numbers of images per class {}\".format(class_count) \n",
    "    #print \"Found the following numbers of images per driver {}\".format(driver_count)\n",
    "    print \"Total number of images found {}\".format(total)\n",
    "    #Return a list of images and their classification\n",
    "    return image_list, class_list\n",
    "\n",
    "# From https://groups.google.com/forum/#!topic/caffe-users/SqYFtnNVNSU\n",
    "def write_image_file(location, image_list, class_list):\n",
    "    filename = location + \".txt\"\n",
    "    print \"write_image_file()/: Creating image and class list... {}\".format(filename)\n",
    "    f = open(filename, 'w')  # Create a txt file with the image location and class\n",
    "    for i, c in zip(image_list, class_list):\n",
    "        f.write(i + \" \" + str(c) + \"\\n\")\n",
    "    f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying training images:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-26ae88898827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\nIdentifying training images:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_image_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_class_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_image_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_image_source_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#print \"First 20 training images: {}\".format(train_image_list[:20])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print \"First 20 training classes: {}\".format(train_class_list[:20])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"\\nIdentifying training images:\"\n",
    "train_image_list, train_class_list = split_image_data(train_image_source_dir, driver_list, training_list)\n",
    "#print \"First 20 training images: {}\".format(train_image_list[:20])\n",
    "#print \"First 20 training classes: {}\".format(train_class_list[:20])\n",
    "write_image_file(train_images, train_image_list, train_class_list)\n",
    "\n",
    "print \"\\nIdentifying validation images:\"\n",
    "validation_image_list, validation_class_list = split_image_data(train_image_source_dir, driver_list, validation_list)\n",
    "#print \"First 20 validation images: {}\".format(validation_image_list[:20])\n",
    "#print \"First 20 validation classes: {}\".format(validation_class_list[:20])\n",
    "write_image_file(valid_images, validation_image_list, validation_class_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an lmdb database for each of the sets of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing train_lmdb\n",
      "Removing existing validate_lmdb\n",
      "Creating train lmdb...\n",
      "I0712 01:20:52.640841 1964806144 convert_imageset.cpp:86] Shuffling data\n",
      "I0712 01:20:52.901465 1964806144 convert_imageset.cpp:89] A total of 16566 images.\n",
      "I0712 01:20:52.902842 1964806144 db_lmdb.cpp:35] Opened lmdb data/state-farm/train_lmdb\n",
      "I0712 01:21:03.558426 1964806144 convert_imageset.cpp:147] Processed 1000 files.\n",
      "I0712 01:21:14.932417 1964806144 convert_imageset.cpp:147] Processed 2000 files.\n",
      "I0712 01:21:26.296730 1964806144 convert_imageset.cpp:147] Processed 3000 files.\n",
      "I0712 01:21:35.758735 1964806144 convert_imageset.cpp:147] Processed 4000 files.\n",
      "I0712 01:21:46.864233 1964806144 convert_imageset.cpp:147] Processed 5000 files.\n",
      "I0712 01:21:57.648986 1964806144 convert_imageset.cpp:147] Processed 6000 files.\n",
      "I0712 01:22:09.881687 1964806144 convert_imageset.cpp:147] Processed 7000 files.\n",
      "I0712 01:22:19.843945 1964806144 convert_imageset.cpp:147] Processed 8000 files.\n",
      "I0712 01:22:32.464056 1964806144 convert_imageset.cpp:147] Processed 9000 files.\n",
      "I0712 01:22:42.699789 1964806144 convert_imageset.cpp:147] Processed 10000 files.\n",
      "I0712 01:22:55.349544 1964806144 convert_imageset.cpp:147] Processed 11000 files.\n",
      "I0712 01:23:05.709134 1964806144 convert_imageset.cpp:147] Processed 12000 files.\n",
      "I0712 01:23:17.364487 1964806144 convert_imageset.cpp:147] Processed 13000 files.\n",
      "I0712 01:23:29.947621 1964806144 convert_imageset.cpp:147] Processed 14000 files.\n",
      "I0712 01:23:42.282904 1964806144 convert_imageset.cpp:147] Processed 15000 files.\n",
      "I0712 01:23:52.065155 1964806144 convert_imageset.cpp:147] Processed 16000 files.\n",
      "I0712 01:23:57.800876 1964806144 convert_imageset.cpp:153] Processed 16566 files.\n",
      "Creating validate lmdb...\n",
      "I0712 01:23:57.851322 1964806144 convert_imageset.cpp:86] Shuffling data\n",
      "I0712 01:23:58.134593 1964806144 convert_imageset.cpp:89] A total of 5858 images.\n",
      "I0712 01:23:58.136006 1964806144 db_lmdb.cpp:35] Opened lmdb data/state-farm/validate_lmdb\n",
      "I0712 01:24:09.306355 1964806144 convert_imageset.cpp:147] Processed 1000 files.\n",
      "I0712 01:24:19.352373 1964806144 convert_imageset.cpp:147] Processed 2000 files.\n",
      "I0712 01:24:29.339053 1964806144 convert_imageset.cpp:147] Processed 3000 files.\n",
      "I0712 01:24:39.006614 1964806144 convert_imageset.cpp:147] Processed 4000 files.\n",
      "I0712 01:24:48.731993 1964806144 convert_imageset.cpp:147] Processed 5000 files.\n",
      "I0712 01:24:59.267696 1964806144 convert_imageset.cpp:153] Processed 5858 files.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!scripts/create_lmdb_from_images.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inital CNN based on the AlexNet example\n",
    "Starting with no pre-loaded weights though as we'll train this with our own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#caffe.set_device(0)     #Needed for GPUs if we have multiple\n",
    "caffe.set_mode_cpu()    \n",
    "\n",
    "model_def = './model/state-farm-train-test.prototxt'\n",
    "model_weights = './model/caffenet-kaggle-state-farm.caffemodel'\n",
    "\n",
    "net = caffe.Net(model_def,      # defines the structure of the model\n",
    "                #model_weights,  # contains the trained weights\n",
    "                caffe.TEST)     # use test mode (e.g., don't perform dropout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LMDBs for training and validating\n",
    "We'll look at testing later on, but this function is for any list of images/classes independent of purpose.\n",
    "From https://github.com/BVLC/caffe/issues/1698#issuecomment-70211045 and http://deepdish.io/2015/04/28/creating-lmdb-in-python/\n",
    "\n",
    "### TODO : Look at transforming the image into the right format now!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_and_resize(): original image shape = (480, 640, 3) and size = 921600\n",
      "load_and_resize(): transformed image shape = (3, 227, 227), size = 154587, datatype <type 'numpy.float32'>\n",
      "[  2.29096928e+01   2.40000000e+01   2.24515419e+01   2.25572701e+01\n",
      "   2.30828915e+01   2.44794521e+01   3.00000000e+01   2.80000000e+01\n",
      "   2.70000000e+01   2.52841415e+01   2.45004215e+01   3.05374451e+01\n",
      "   3.10000000e+01   3.10000000e+01   3.30000000e+01   2.97995586e+01\n",
      "   3.00396481e+01   3.17958584e+01   3.38854637e+01   3.39207077e+01\n",
      "   3.58470154e+01   3.97717667e+01   3.88566322e+01   3.09965458e+01\n",
      "   2.71266632e+01   8.72521782e+00   1.84100552e+01   1.93652458e+01\n",
      "   7.06380415e+00   6.14530182e+00   6.51060104e+00   8.90804231e-01\n",
      "   3.92704606e+00   2.82316767e-02   6.75895596e+00   1.32819390e+00\n",
      "   1.40325060e+01   3.97312508e+01   5.75873337e+01   8.05857010e+01\n",
      "   8.59877777e+01   8.94388351e+01   8.95031052e+01   8.72770462e+01\n",
      "   9.38790894e+01   9.97924042e+01   2.07694260e+02   1.30373688e+02\n",
      "   1.33055077e+01   1.00951546e+02   1.08574020e+02   1.30110214e+02\n",
      "   1.66936310e+02   1.84472534e+02   1.81102493e+02   1.79525085e+02\n",
      "   1.69443497e+02   1.74442657e+02   2.05239914e+02   2.49666214e+00\n",
      "   2.44768543e+01   3.71172791e+01   3.75990829e+01   3.65308380e+01\n",
      "   3.76379013e+01   4.10000000e+01   4.10000000e+01   4.10000000e+01\n",
      "   4.10000000e+01   4.20000000e+01   4.30000000e+01   4.40000000e+01\n",
      "   4.50000000e+01   4.60000000e+01   4.89611320e+01   5.81783028e+01\n",
      "   7.42081070e+01   9.71275024e+01   1.66859634e+02   2.08947128e+02\n",
      "   2.07266525e+02   2.09557266e+02   2.10919525e+02   1.28616226e+02\n",
      "   6.43721771e+01   2.25835526e+02   1.69352066e+02   2.43525711e+02\n",
      "   2.52006821e+02   2.53000000e+02   2.51000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.53747437e+02   2.52167404e+02   2.49997940e+02   2.47257706e+02\n",
      "   2.51367828e+02   2.55000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.55000000e+02   2.54932709e+02   2.49614532e+02\n",
      "   2.49997940e+02   2.52167404e+02   2.51839203e+02   2.51138702e+02\n",
      "   2.53068268e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.55000000e+02\n",
      "   2.55000000e+02   2.55000000e+02   2.55000000e+02   2.55000000e+02\n",
      "   2.55000000e+02   2.55000000e+02   2.55000000e+02   2.55000000e+02\n",
      "   2.55000000e+02   2.55000000e+02   2.55000000e+02   2.55000000e+02\n",
      "   2.55000000e+02   2.55000000e+02   2.55000000e+02   2.55000000e+02\n",
      "   2.55000000e+02   2.55000000e+02   2.55000000e+02   2.55000000e+02\n",
      "   2.55000000e+02   2.55000000e+02   2.55000000e+02   2.55000000e+02\n",
      "   2.55000000e+02   2.55000000e+02   2.55000000e+02   2.55000000e+02\n",
      "   2.55000000e+02   2.55000000e+02   2.55000000e+02   2.55000000e+02\n",
      "   2.55000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.54000000e+02   2.54000000e+02\n",
      "   2.52000000e+02   2.52000000e+02   2.52000000e+02   2.54000000e+02\n",
      "   2.54000000e+02   2.54000000e+02   2.53431686e+02   2.50167099e+02\n",
      "   2.50219864e+02   2.49178864e+02   2.49316757e+02   2.50566071e+02\n",
      "   2.53666473e+02   2.52355377e+02   2.54320312e+02   2.54714264e+02\n",
      "   2.55000000e+02   2.55000000e+02   2.53280319e+02   2.50253311e+02\n",
      "   2.52000000e+02   2.55000000e+02   2.55000000e+02]\n",
      "Writing 0000000000, channels 3, height 227, width 227, data size 618348\n"
     ]
    }
   ],
   "source": [
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "#transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR\n",
    "\n",
    "def load_and_resize_image(filename, transform = False):\n",
    "    image = caffe.io.load_image(filename)            \n",
    "    #image = caffe.io.resize_image(image, (120, 160), interp_order = 3 )\n",
    "    print \"load_and_resize(): original image shape = {} and size = {}\".format(image.shape, image.size)\n",
    "    if transform == True:\n",
    "        # BGR (switch from RGB)\n",
    "        # Channel x Height x Width order (switch from H x W x C)\n",
    "        # TODO NEXT --> np.uint8 {0, ..., 255}\n",
    "        #image = image[:,:,::-1]\n",
    "        #image = image.transpose((2,0,1))\n",
    "        image = transformer.preprocess('data', image)\n",
    "        print \"load_and_resize(): transformed image shape = {}, size = {}, datatype {}\".format(image.shape, image.size, type(image[0][0][0]))\n",
    "    return image\n",
    "    \n",
    "def create_lmdb(filename, image_list, class_list):\n",
    "    img = {}\n",
    "    #First create the lmdb with the maximum size (it doesn't create this size, just sets the maximum!)\n",
    "    image_db = lmdb.open(filename, map_size=int(1e12))\n",
    "    \n",
    "    #Now iterate the image list, load, resize and store \n",
    "    #Note we may need to chunk this into multiple transactions if there are issues with a single txn\n",
    "    with image_db.begin(write=True) as txn:\n",
    "        for index, img_filename in enumerate(image_list):\n",
    "            datum = caffe.proto.caffe_pb2.Datum()\n",
    "            #Load image\n",
    "            image = load_and_resize_image(img_filename, transform = True) \n",
    "            datum.channels = image.shape[0]\n",
    "            datum.height = image.shape[1]\n",
    "            datum.width = image.shape[2]\n",
    "            print image[0][0]\n",
    "            datum.data = image.tobytes()\n",
    "            datum.label = int(class_list[index])\n",
    "            print 'Writing {:0>10d}, channels {}, height {}, width {}, data size {}'.format(\n",
    "                    index, datum.channels, datum.height, datum.width, len(datum.data))\n",
    "            #print 'datum.data: {}'.format(datum.data)\n",
    "            #print 'datum.SerializeToString() {}'.format(datum.SerializeToString())\n",
    "            txn.put('{:0>10d}'.format(index), datum.SerializeToString())\n",
    "    image_db.close()\n",
    "\n",
    "#TODO - Check whether the file exists before recreating it??  \n",
    "#Using head(10) for now to prove it works!!\n",
    "create_lmdb('model/state-farm-train.lmdb', train_image_list[:1], train_class_list[:1])\n",
    "#create_lmdb('model/state-farm-validation.lmdb', validation_image_list, validation_class_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR Try to set up a Lenet Mnist model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P\n",
    "\n",
    "def lenet(lmdb, batch_size):\n",
    "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,\n",
    "                             transform_param=dict(scale=1./255), ntop=2)\n",
    "    \n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))\n",
    "    n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))\n",
    "    n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n",
    "    n.relu1 = L.ReLU(n.fc1, in_place=True)\n",
    "    n.score = L.InnerProduct(n.relu1, num_output=10, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.SoftmaxWithLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()\n",
    "    \n",
    "with open('model/lenet_auto_train.prototxt', 'w') as f:\n",
    "    f.write(str(lenet('model/mnist_train_lmdb', 64)))\n",
    "    \n",
    "with open('model/lenet_auto_validate.prototxt', 'w') as f:\n",
    "    f.write(str(lenet('model/mnist_validate_lmdb', 100)))\n",
    "    \n",
    "with open('model/lenet_auto_test.prototxt', 'w') as f:\n",
    "    f.write(str(lenet('model/mnist_test_lmdb', 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver = caffe.SGDSolver('./model/lenet_auto_train.prototxt')\n",
    "\n",
    "# just print the weight sizes (we'll omit the biases)\n",
    "#[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the architecture of our net, we can check the dimensions of the intermediate features (blobs) and parameters (these will also be useful to refer to when manipulating data later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'solver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6ab9188daa7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# just print the weight sizes (we'll omit the biases)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'solver' is not defined"
     ]
    }
   ],
   "source": [
    "# just print the weight sizes (we'll omit the biases)\n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
