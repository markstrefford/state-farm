{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the State Farm image data\n",
    "\n",
    "This notebook provides analysis of the provided State Farm data, using Theano and Keras to build the NN.\n",
    "\n",
    "Note that the data is available from Kaggle here:  \n",
    "https://www.kaggle.com/c/state-farm-distracted-driver-detection/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "First, let's import what we need and set up environment variables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Can't change the value of this config parameter after initialization!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e246e7cb99d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/configparser.pyc\u001b[0m in \u001b[0;36m__set__\u001b[0;34m(self, cls, val)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_override\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             raise Exception(\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0;34m\"Can't change the value of this config parameter \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \"after initialization!\")\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# print \"SETTING PARAM\", self.fullname,(cls), val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Can't change the value of this config parameter after initialization!"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "theano.config.device = 'gpu'\n",
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports of the relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# These are the locations of the images provided by Kaggle\n",
    "# Root Dir is needed for Python, but not for create lmdb shell script later... (we need it there too!)\n",
    "image_root_dir = './imgs/'\n",
    "train_image_source_dir = \"./train/\"\n",
    "test_image_source_dir = \"./test/\"\n",
    "driver_image_list = \"./driver_imgs_list.csv\"\n",
    "\n",
    "# These are the locations of the images that we will work with \n",
    "# Note that as we're continually mix up training and validation drivers/images, \n",
    "# then we will store images in one directory and use code to determine whether to train or validate\n",
    "train_images_dir = \"./images/train/\"\n",
    "#validation_images_dir = \"./images/validate/\" \n",
    "test_images_dir = \"./images/test/\"\n",
    "\n",
    "# Some more controls\n",
    "# color type: 1 - grey, 3 - rgb\n",
    "color_type = 1 \n",
    "image_width = 224 #80\n",
    "image_height = 224 #60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by pre-processing the images\n",
    "There are only 27 different drivers so in order to avoid overfitting, or testing using very similar data to training, we will split the data based on the driver into train and validation sets.\n",
    "\n",
    "Initially though, let's get the list of drivers, see how many images are available for each driver, and which classification they have been labelled with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data summary: \n",
      "  subject classname            img\n",
      "0    p002        c0  img_44733.jpg\n",
      "1    p002        c0  img_72999.jpg\n",
      "2    p002        c0  img_25094.jpg\n",
      "3    p002        c0  img_69092.jpg\n",
      "4    p002        c0  img_92629.jpg\n",
      "\n",
      "Testing data summary: \n",
      "['img_1.jpg', 'img_10.jpg', 'img_100.jpg', 'img_1000.jpg', 'img_100000.jpg', 'img_100001.jpg', 'img_100002.jpg', 'img_100003.jpg', 'img_100004.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Training set is in the provided csv file\n",
    "driver_list = pd.read_csv(driver_image_list)\n",
    "print \"Training data summary: \\n{}\".format(driver_list.head())\n",
    "\n",
    "test_image_list = os.listdir(image_root_dir + test_image_source_dir)\n",
    "print \"\\nTesting data summary: \\n{}\".format(test_image_list[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process images so that they are in an format more suited to training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images found 22424\n"
     ]
    }
   ],
   "source": [
    "def get_driver_images_and_classes(driver_list):\n",
    "    image_list = []\n",
    "    class_list = []\n",
    "    total = 0\n",
    "    for driver_row in [ drvr for drvr in driver_list.iterrows() ]:   # if drvr[1]['subject'] in filter \n",
    "        driver = driver_row[1]  # Drop the index created by the Pandas Dataframe\n",
    "        driver_class = int(driver['classname'][1:])  # Get integer to represent class (eg 'c0' is class '0')\n",
    "        image_list.append(driver['img'])\n",
    "        class_list.append(driver_class)\n",
    "        total += 1\n",
    "    print \"Total number of training images found {}\".format(total)\n",
    "    #Return a list of images and their classification\n",
    "    return np.array(image_list), np.array(class_list)\n",
    "\n",
    "# Create a training list of images and classes from the training set\n",
    "images, classes = get_driver_images_and_classes(driver_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Process the image, for now this is resize only\n",
    "# We'll handle colour/greyscale when we load as cv2 does this for us\n",
    "# TODO - Move directory creation to Python code to be OS independent\n",
    "\n",
    "def pre_process_image(image):\n",
    "    processed_img = cv2.resize(image, (image_width, image_height)) \n",
    "    return processed_img\n",
    "    \n",
    "def create_train_image_repository(images_dest_dir, images_list, class_list, color_type=1):\n",
    "    print \"create_image_repository(): Processing images into {}\".format(images_dest_dir)\n",
    "    copied = 0 \n",
    "    for f, c in zip(images_list, class_list):\n",
    "        dest_dir = images_dest_dir + str(c) + \"/\"\n",
    "        if os.path.exists(dest_dir) == False:\n",
    "            !mkdir $dest_dir\n",
    "        image_filename = image_root_dir + train_image_source_dir + '/c' + str(c) + '/' + f\n",
    "        if color_type == 1:\n",
    "            orig_img = cv2.imread(image_filename, 0)\n",
    "        elif color_type == 3:\n",
    "            orig_img = cv2.imread(image_filename)\n",
    "        processed_image = pre_process_image(orig_img)\n",
    "        cv2.imwrite(images_dest_dir + str(c) + \"/\" + f, processed_image)\n",
    "        copied += 1\n",
    "        if copied % 100 == 0:\n",
    "            print \".\",\n",
    "    print \"\\nCopied {} images...Done!\".format(copied)\n",
    "    return copied\n",
    "\n",
    "def create_test_image_repository(dest_dir, images_list, color_type=1):\n",
    "    print \"create_image_repository(): Processing images into {}\".format(dest_dir)\n",
    "    copied = 0 \n",
    "    for f in images_list:\n",
    "        if os.path.exists(dest_dir) == False:\n",
    "            !mkdir $dest_dir\n",
    "        image_filename = image_root_dir + test_image_source_dir + f\n",
    "        if color_type == 1:\n",
    "            orig_img = cv2.imread(image_filename, 0)\n",
    "        elif color_type == 3:\n",
    "            orig_img = cv2.imread(image_filename)\n",
    "        processed_image = pre_process_image(orig_img)\n",
    "        cv2.imwrite(dest_dir + f, processed_image)\n",
    "        copied += 1\n",
    "        if copied % 100 == 0:\n",
    "            print \".\",\n",
    "    print \"\\nCopied {} images...Done!\".format(copied)\n",
    "    return copied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process images if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start by clearing out any old data (ignore failures here if the directory doesn't exist)\n",
    "# TODO - Move to Python code to be OS independent\n",
    "\n",
    "create_repository = False    # True forces creation of the processed images, \n",
    "                            # Set to False if this has been done previously\n",
    "if create_repository:\n",
    "    print \"Deleting old repositories if they exist, this may take a while...\"\n",
    "    !rm -rf $train_images_dir\n",
    "    #!rm -rf $validation_images_dir\n",
    "    !rm -rf $test_images_dir\n",
    "\n",
    "    # Create directories\n",
    "    !mkdir -p $train_images_dir\n",
    "    #!mkdir -p $validation_images_dir\n",
    "    !mkdir -p $test_images_dir\n",
    "\n",
    "    create_test_image_repository(test_images_dir, test_image_list, color_type=color_type)\n",
    "    create_train_image_repository(train_images_dir, images, classes, color_type=color_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and validation data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the drivers into a training and validation set.  To ensure we don't have overfitting (the training set and the validation set contain the same or similar images) we will split on drivers, so a driver can only appear in training or validation but not both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 drivers: ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081']\n"
     ]
    }
   ],
   "source": [
    "driver_ids = []\n",
    "for id, driver in driver_list.iterrows():\n",
    "    if driver['subject'] not in driver_ids:\n",
    "        driver_ids.append(driver['subject'])\n",
    "print \"Found {} drivers: {}\".format(len(driver_ids), driver_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def render_image(image_filename):\n",
    "    print \"render_image(): Rendering {}\".format(image_filename)\n",
    "    image = cv2.imread(image_filename, color_type_global)\n",
    "    plt.axis(\"off\")\n",
    "    #plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.imshow(image)\n",
    "    plt.show() \n",
    "    #print image.shape\n",
    "    #plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and validation data tests (split = percentage to have in training set)\n",
    "and then create X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "def create_train_validation_data(driver_list, filter):\n",
    "    #sample = driver_list[driver_list.subject.isin(filter)].ix[:, 'classname':'img']\n",
    "    images = []\n",
    "    labels = []\n",
    "    total = 0\n",
    "    for driver_row in [ drvr for drvr in driver_list[driver_list.subject.isin(filter)].ix[:, 'classname':'img'].iterrows() ]:   # if drvr[1]['subject'] in filter \n",
    "        driver = driver_row[1]  # Drop the index created by the Pandas Dataframe\n",
    "        #print driver\n",
    "        label = int(driver['classname'][1:])\n",
    "        filename = train_images_dir + str(label) + \"/\" + driver['img']\n",
    "        if color_type == 1:\n",
    "            image = cv2.imread(filename, 0).transpose()  # Is the color_type needed here as these are pre-processed images??\n",
    "        elif color_type == 3:\n",
    "            image = cv2.imread(filename).transpose()     # Is the color_type needed here as these are pre-processed images??\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "        total += 1\n",
    "        if total % 100 == 0:\n",
    "            print \".\",\n",
    "    print \"\\nProcessed {} rows.\".format(total)\n",
    "    \n",
    "    images = np.array(images, dtype=np.uint8)\n",
    "    images = images.reshape(images.shape[0], color_type, image_width, image_height)\n",
    "    images = images.astype('float32')\n",
    "    images /= 255\n",
    "    \n",
    "    labels = np.array(labels, dtype=np.uint8)\n",
    "    labels = np_utils.to_categorical(labels, 10)\n",
    "\n",
    "    return images, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver train list: ['p051' 'p075' 'p035' 'p050' 'p066' 'p022' 'p041' 'p012' 'p026' 'p061'\n",
      " 'p056' 'p002' 'p039' 'p081' 'p042' 'p047' 'p064' 'p024' 'p021' 'p015'\n",
      " 'p049' 'p016' 'p045' 'p014' 'p052' 'p072']\n",
      "Driver validation list: []\n"
     ]
    }
   ],
   "source": [
    "def split_drivers_into_train_and_validate(driver_list, split = 1):\n",
    "    driver_valid_list = []\n",
    "    # Take a random sample of drivers into the training list\n",
    "    driver_train_list = np.random.choice(driver_list, int(len(driver_list)*split), replace = False)\n",
    "    # Take the remaining drivers into the validation list\n",
    "    driver_valid_list = [ driver for driver in driver_list if driver not in driver_train_list]\n",
    "    return driver_train_list, driver_valid_list\n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_all.ix[rows], student_data[target_col].ix[rows], test_size=test_size)\n",
    "\n",
    "training_list, validation_list = split_drivers_into_train_and_validate(driver_ids)\n",
    "print \"Driver train list: {}\".format(training_list)\n",
    "print \"Driver validation list: {}\".format(validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "Processed 22424 rows.\n",
      "Creating validation data:\n",
      "\n",
      "Processed 0 rows.\n"
     ]
    }
   ],
   "source": [
    "# TODO - Add in random ordering of training data!!\n",
    "# index = np.random.choice(range(0, num_training_samples), num_training_samples, replace = False) # Random ordering\n",
    "# ...driver_list[index], training_list[index]\n",
    "print \"Creating training data:\"\n",
    "X_train, y_train = create_train_validation_data(driver_list, training_list)\n",
    "print \"Creating validation data:\"\n",
    "X_valid, y_valid = create_train_validation_data(driver_list, validation_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22424, 1, 224, 224)\n",
      "(22424, 10)\n",
      "(0, 1, 224, 224)\n",
      "(0, 10)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "num_training_samples = X_train.shape[0]\n",
    "print X_valid.shape\n",
    "print y_valid.shape\n",
    "num_validation_samples = X_valid.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inital CNN using Keras\n",
    "Starting with no pre-loaded weights though as we'll train this with our own data.\n",
    "Based on example here http://keras.io\n",
    "\n",
    "TODO: In a future iteration, we'll play about with this architecture and the activation, optimizer and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D  \n",
    "from keras.layers import Dense, Flatten, Activation, Dropout, MaxPooling1D\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Custom Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def custom_keras_model(num_classes, weights_path=None, w_regularizer = None, b_regularizer = None):\n",
    "    num_filters = 8      #number of filters to apply/learn in the 1D convolutional layer\n",
    "    num_pooling = 2\n",
    "    filter_length = 5     #linear length of each filter (this is 1D)\n",
    "    num_filters_2 = 8\n",
    "\n",
    "    #num_filters = 8      #number of filters to apply/learn in the 1D convolutional layer\n",
    "    #num_pooling = 2\n",
    "    #filter_length = 2     #linear length of each filter (this is 1D)\n",
    "    #num_filters_2 = 16\n",
    "    \n",
    "    # Create callback for history report\n",
    "    from keras.callbacks import Callback\n",
    "    class LossHistory(Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "    \n",
    "    \n",
    "    #from keras.utils.dot_utils import Grapher\n",
    "    \n",
    "    model = Sequential()\n",
    "    #grapher = Grapher()\n",
    "\n",
    "    # Now create the NN architecture (version 1)\n",
    "    # Going with colour for now!!\n",
    "    model.add(Convolution2D(num_filters, filter_length, filter_length, border_mode=\"valid\", \n",
    "                        activation=\"relu\", \n",
    "                        W_regularizer = w_regularizer, b_regularizer = b_regularizer,\n",
    "                        input_shape=(color_type, image_width, image_height)))\n",
    "    \n",
    "    # Added\n",
    "    model.add(MaxPooling2D(pool_size=(num_pooling, num_pooling)))  \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Convolution2D(num_filters_2, filter_length, filter_length, \n",
    "                            W_regularizer = w_regularizer, b_regularizer = b_regularizer))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(num_pooling, num_pooling)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, \n",
    "              W_regularizer = w_regularizer, b_regularizer = b_regularizer))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, \n",
    "              W_regularizer = w_regularizer, b_regularizer = b_regularizer))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    if weights_path:\n",
    "        print \"Loading weights from {}\".format(weights_path)\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "\n",
    "    #model.summary()\n",
    "    #grapher.plot(model, 'nn_model.png')\n",
    "    \n",
    "    # TODO - Handle loading existing weights \n",
    "    \n",
    "    return model, LossHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine which NN we are going to use, and whether to load weights or train ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def graph_training_loss_history(losses):\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(losses)\n",
    "    plt.ylabel('error')\n",
    "    plt.xlabel('batch')\n",
    "    plt.title('training error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "def get_log_loss_score(model, X_valid, y_valid):\n",
    "    predictions_valid = model.predict(X_valid, batch_size=128, verbose=1)\n",
    "    score = log_loss(y_valid, predictions_valid)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Configure the network\n",
    "def compile_model(learning_rate=0.1):\n",
    "    if keras_model == 'custom':\n",
    "        model, LossHistory = custom_keras_model(num_classes, weights, w_regularizer, b_regularizer)\n",
    "        sgd = SGD(lr=learning_rate, decay=0, momentum=0, nesterov=False)\n",
    "        #sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    elif keras_model == 'vgg16':\n",
    "        model, LossHistory = vgg16(num_classes, weights)\n",
    "        sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    # Now compile the model\n",
    "    model.compile(loss=loss_function, optimizer=sgd, metrics=['accuracy'])\n",
    "    return model, LossHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras_model, weights, train_model = 'custom', None, True\n",
    "#keras_model, weights, train_model = 'vgg16', 'model/vgg16_weights.h5', False\n",
    "loss_function='categorical_crossentropy'\n",
    "from keras.regularizers import l1, l2\n",
    "w_regularizer = l2(0.1) # Default = None\n",
    "b_regularizer = l2(0.1) # Default = None\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 8, 220, 220)   208         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 8, 110, 110)   0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 8, 110, 110)   0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 8, 106, 106)   1608        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 8, 106, 106)   0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 8, 53, 53)     0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 8, 53, 53)     0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 22472)         0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           2876544     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 128)           0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 10)            1290        dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 10)            0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2879650\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"994pt\" viewBox=\"0.00 0.00 229.68 994.00\" width=\"230pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 990)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-990 225.68,-990 225.68,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 4524138192 -->\n",
       "<g class=\"node\" id=\"node1\"><title>4524138192</title>\n",
       "<polygon fill=\"none\" points=\"0,-949.5 0,-985.5 221.68,-985.5 221.68,-949.5 0,-949.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-963.3\">convolution2d_input_1 (InputLayer)</text>\n",
       "</g>\n",
       "<!-- 4524135952 -->\n",
       "<g class=\"node\" id=\"node2\"><title>4524135952</title>\n",
       "<polygon fill=\"none\" points=\"5.42773,-876.5 5.42773,-912.5 216.252,-912.5 216.252,-876.5 5.42773,-876.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-890.3\">convolution2d_1 (Convolution2D)</text>\n",
       "</g>\n",
       "<!-- 4524138192&#45;&gt;4524135952 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>4524138192-&gt;4524135952</title>\n",
       "<path d=\"M110.84,-949.313C110.84,-941.289 110.84,-931.547 110.84,-922.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-922.529 110.84,-912.529 107.34,-922.529 114.34,-922.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4511239248 -->\n",
       "<g class=\"node\" id=\"node3\"><title>4511239248</title>\n",
       "<polygon fill=\"none\" points=\"5.81738,-803.5 5.81738,-839.5 215.862,-839.5 215.862,-803.5 5.81738,-803.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-817.3\">maxpooling2d_1 (MaxPooling2D)</text>\n",
       "</g>\n",
       "<!-- 4524135952&#45;&gt;4511239248 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>4524135952-&gt;4511239248</title>\n",
       "<path d=\"M110.84,-876.313C110.84,-868.289 110.84,-858.547 110.84,-849.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-849.529 110.84,-839.529 107.34,-849.529 114.34,-849.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4511364752 -->\n",
       "<g class=\"node\" id=\"node4\"><title>4511364752</title>\n",
       "<polygon fill=\"none\" points=\"44.3208,-730.5 44.3208,-766.5 177.359,-766.5 177.359,-730.5 44.3208,-730.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-744.3\">dropout_1 (Dropout)</text>\n",
       "</g>\n",
       "<!-- 4511239248&#45;&gt;4511364752 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>4511239248-&gt;4511364752</title>\n",
       "<path d=\"M110.84,-803.313C110.84,-795.289 110.84,-785.547 110.84,-776.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-776.529 110.84,-766.529 107.34,-776.529 114.34,-776.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4511902288 -->\n",
       "<g class=\"node\" id=\"node5\"><title>4511902288</title>\n",
       "<polygon fill=\"none\" points=\"5.42773,-657.5 5.42773,-693.5 216.252,-693.5 216.252,-657.5 5.42773,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-671.3\">convolution2d_2 (Convolution2D)</text>\n",
       "</g>\n",
       "<!-- 4511364752&#45;&gt;4511902288 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>4511364752-&gt;4511902288</title>\n",
       "<path d=\"M110.84,-730.313C110.84,-722.289 110.84,-712.547 110.84,-703.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-703.529 110.84,-693.529 107.34,-703.529 114.34,-703.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4511980880 -->\n",
       "<g class=\"node\" id=\"node6\"><title>4511980880</title>\n",
       "<polygon fill=\"none\" points=\"32.2793,-584.5 32.2793,-620.5 189.4,-620.5 189.4,-584.5 32.2793,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-598.3\">activation_1 (Activation)</text>\n",
       "</g>\n",
       "<!-- 4511902288&#45;&gt;4511980880 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>4511902288-&gt;4511980880</title>\n",
       "<path d=\"M110.84,-657.313C110.84,-649.289 110.84,-639.547 110.84,-630.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-630.529 110.84,-620.529 107.34,-630.529 114.34,-630.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4511967888 -->\n",
       "<g class=\"node\" id=\"node7\"><title>4511967888</title>\n",
       "<polygon fill=\"none\" points=\"5.81738,-511.5 5.81738,-547.5 215.862,-547.5 215.862,-511.5 5.81738,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-525.3\">maxpooling2d_2 (MaxPooling2D)</text>\n",
       "</g>\n",
       "<!-- 4511980880&#45;&gt;4511967888 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>4511980880-&gt;4511967888</title>\n",
       "<path d=\"M110.84,-584.313C110.84,-576.289 110.84,-566.547 110.84,-557.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-557.529 110.84,-547.529 107.34,-557.529 114.34,-557.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4511968336 -->\n",
       "<g class=\"node\" id=\"node8\"><title>4511968336</title>\n",
       "<polygon fill=\"none\" points=\"44.3208,-438.5 44.3208,-474.5 177.359,-474.5 177.359,-438.5 44.3208,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-452.3\">dropout_2 (Dropout)</text>\n",
       "</g>\n",
       "<!-- 4511967888&#45;&gt;4511968336 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>4511967888-&gt;4511968336</title>\n",
       "<path d=\"M110.84,-511.313C110.84,-503.289 110.84,-493.547 110.84,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-484.529 110.84,-474.529 107.34,-484.529 114.34,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4512348176 -->\n",
       "<g class=\"node\" id=\"node9\"><title>4512348176</title>\n",
       "<polygon fill=\"none\" points=\"52.4897,-365.5 52.4897,-401.5 169.19,-401.5 169.19,-365.5 52.4897,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-379.3\">flatten_1 (Flatten)</text>\n",
       "</g>\n",
       "<!-- 4511968336&#45;&gt;4512348176 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>4511968336-&gt;4512348176</title>\n",
       "<path d=\"M110.84,-438.313C110.84,-430.289 110.84,-420.547 110.84,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-411.529 110.84,-401.529 107.34,-411.529 114.34,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4513522384 -->\n",
       "<g class=\"node\" id=\"node10\"><title>4513522384</title>\n",
       "<polygon fill=\"none\" points=\"55.9966,-292.5 55.9966,-328.5 165.683,-328.5 165.683,-292.5 55.9966,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-306.3\">dense_1 (Dense)</text>\n",
       "</g>\n",
       "<!-- 4512348176&#45;&gt;4513522384 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>4512348176-&gt;4513522384</title>\n",
       "<path d=\"M110.84,-365.313C110.84,-357.289 110.84,-347.547 110.84,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-338.529 110.84,-328.529 107.34,-338.529 114.34,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4513524176 -->\n",
       "<g class=\"node\" id=\"node11\"><title>4513524176</title>\n",
       "<polygon fill=\"none\" points=\"32.2793,-219.5 32.2793,-255.5 189.4,-255.5 189.4,-219.5 32.2793,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-233.3\">activation_2 (Activation)</text>\n",
       "</g>\n",
       "<!-- 4513522384&#45;&gt;4513524176 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>4513522384-&gt;4513524176</title>\n",
       "<path d=\"M110.84,-292.313C110.84,-284.289 110.84,-274.547 110.84,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-265.529 110.84,-255.529 107.34,-265.529 114.34,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4513525200 -->\n",
       "<g class=\"node\" id=\"node12\"><title>4513525200</title>\n",
       "<polygon fill=\"none\" points=\"44.3208,-146.5 44.3208,-182.5 177.359,-182.5 177.359,-146.5 44.3208,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-160.3\">dropout_3 (Dropout)</text>\n",
       "</g>\n",
       "<!-- 4513524176&#45;&gt;4513525200 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>4513524176-&gt;4513525200</title>\n",
       "<path d=\"M110.84,-219.313C110.84,-211.289 110.84,-201.547 110.84,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-192.529 110.84,-182.529 107.34,-192.529 114.34,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4513637712 -->\n",
       "<g class=\"node\" id=\"node13\"><title>4513637712</title>\n",
       "<polygon fill=\"none\" points=\"55.9966,-73.5 55.9966,-109.5 165.683,-109.5 165.683,-73.5 55.9966,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-87.3\">dense_2 (Dense)</text>\n",
       "</g>\n",
       "<!-- 4513525200&#45;&gt;4513637712 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>4513525200-&gt;4513637712</title>\n",
       "<path d=\"M110.84,-146.313C110.84,-138.289 110.84,-128.547 110.84,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-119.529 110.84,-109.529 107.34,-119.529 114.34,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4513567120 -->\n",
       "<g class=\"node\" id=\"node14\"><title>4513567120</title>\n",
       "<polygon fill=\"none\" points=\"32.2793,-0.5 32.2793,-36.5 189.4,-36.5 189.4,-0.5 32.2793,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.84\" y=\"-14.3\">activation_3 (Activation)</text>\n",
       "</g>\n",
       "<!-- 4513637712&#45;&gt;4513567120 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>4513637712-&gt;4513567120</title>\n",
       "<path d=\"M110.84,-73.3129C110.84,-65.2895 110.84,-55.5475 110.84,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.34,-46.5288 110.84,-36.5288 107.34,-46.5289 114.34,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model so we can get a view of what it looks like\n",
    "# Note we'll do this in each training iteration later, but this'll keep the output tidy!\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "model, LossHistory = compile_model()\n",
    "model.summary()\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "Train and use validation data to see if we're training effectively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************\n",
      "Starting training iteration 1 with learning rate 0.03\n",
      "\n",
      "Train on 19060 samples, validate on 3364 samples\n",
      "Epoch 1/5\n",
      " 1280/19060 [=>............................] - ETA: 439s - loss: 5.6361 - acc: 0.1039"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 5 #10\n",
    "learning_rate = [0.003, 0.01, 0.03, 0.1, 0.3]\n",
    "\n",
    "for i in range(0,1):   # range(0,4) if doing multiple learning rates\n",
    "    #learningrate = learning_rate[i]\n",
    "    learningrate = 0.03\n",
    "    print \"\\n**********************************\"\n",
    "    print \"Starting training iteration {} with learning rate {}\\n\".format(i+1, learningrate)\n",
    "    model, LossHistory = compile_model(learningrate)\n",
    "    history = LossHistory()\n",
    "    #model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=num_epochs,\n",
    "    #          verbose=1, validation_data=(X_valid, y_valid), shuffle=True,\n",
    "    #          callbacks=[history]) \n",
    "    model.fit(X_train, y_train,            \n",
    "              batch_size=batch_size, \n",
    "              nb_epoch=num_epochs,\n",
    "              validation_split = 0.15,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              callbacks=[history])\n",
    "    graph_training_loss_history(history.losses)\n",
    "    #print('Score log_loss: ', get_log_loss_score(model, X_valid, y_valid))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W_constraint': None, 'b_constraint': None, 'name': 'convolution2d_57', 'activity_regularizer': None, 'trainable': True, 'dim_ordering': 'th', 'nb_col': 2, 'subsample': (1, 1), 'init': 'glorot_uniform', 'bias': True, 'nb_filter': 8, 'activation': 'relu', 'input_dtype': 'float32', 'batch_input_shape': (None, 1, 224, 224), 'W_regularizer': None, 'nb_row': 2, 'b_regularizer': {'l2': 0.009999999776482582, 'name': 'WeightRegularizer', 'l1': 0.0}, 'border_mode': 'valid'}\n",
      "[array([[[[  1.00727117e+00,  -4.94124293e-01],\n",
      "         [  3.02565128e-01,   2.76999205e-01]]],\n",
      "\n",
      "\n",
      "       [[[ -2.18588829e-01,   6.67327762e-01],\n",
      "         [ -3.59928757e-01,  -1.37905136e-01]]],\n",
      "\n",
      "\n",
      "       [[[ -9.77293551e-02,  -3.28192919e-01],\n",
      "         [ -2.55384117e-01,   3.11767161e-02]]],\n",
      "\n",
      "\n",
      "       [[[  7.53854573e-01,   4.21378255e-01],\n",
      "         [ -3.35496485e-01,  -6.77961409e-01]]],\n",
      "\n",
      "\n",
      "       [[[  1.22372055e+00,   3.50442290e-01],\n",
      "         [  2.22003549e-01,  -1.48298836e+00]]],\n",
      "\n",
      "\n",
      "       [[[ -2.26020843e-01,  -3.20548207e-01],\n",
      "         [  2.58854687e-01,  -1.46883130e-01]]],\n",
      "\n",
      "\n",
      "       [[[ -2.18074128e-01,   4.01764701e-04],\n",
      "         [  4.59486991e-02,  -1.73615336e-01]]],\n",
      "\n",
      "\n",
      "       [[[ -1.14098108e+00,  -1.39560437e+00],\n",
      "         [  9.08157051e-01,   1.44185495e+00]]]], dtype=float32), array([-0.3160288 , -0.000344  ,  0.00067235, -0.16794719, -0.26512805,\n",
      "        0.00111909, -0.00243902, -0.05782045], dtype=float32)]\n",
      "{'name': 'maxpooling2d_49', 'trainable': True, 'dim_ordering': 'th', 'pool_size': (2, 2), 'strides': (2, 2), 'border_mode': 'valid'}\n",
      "[]\n",
      "{'p': 0.25, 'trainable': True, 'name': 'dropout_76'}\n",
      "[]\n",
      "{'W_constraint': None, 'b_constraint': None, 'name': 'convolution2d_58', 'activity_regularizer': None, 'trainable': True, 'dim_ordering': 'th', 'nb_col': 2, 'subsample': (1, 1), 'init': 'glorot_uniform', 'bias': True, 'nb_filter': 8, 'b_regularizer': {'l2': 0.009999999776482582, 'name': 'WeightRegularizer', 'l1': 0.0}, 'W_regularizer': None, 'nb_row': 2, 'activation': 'linear', 'border_mode': 'valid'}\n",
      "[array([[[[ 0.19804823, -0.30922657],\n",
      "         [ 0.34556508,  0.26594299]],\n",
      "\n",
      "        [[-0.02406395,  0.02286317],\n",
      "         [-0.07196474,  0.10981008]],\n",
      "\n",
      "        [[-0.27436516,  0.01279903],\n",
      "         [ 0.08602349, -0.0692016 ]],\n",
      "\n",
      "        [[ 0.05225278, -0.23813511],\n",
      "         [-0.0889188 , -0.03332487]],\n",
      "\n",
      "        [[ 0.12031033,  0.0993996 ],\n",
      "         [ 0.1009144 , -0.0260751 ]],\n",
      "\n",
      "        [[ 0.21462557,  0.02839359],\n",
      "         [ 0.08883185, -0.29968736]],\n",
      "\n",
      "        [[-0.16241223,  0.26681408],\n",
      "         [ 0.24308804, -0.28261366]],\n",
      "\n",
      "        [[-0.07576084, -0.28213716],\n",
      "         [-0.12594852, -0.1612747 ]]],\n",
      "\n",
      "\n",
      "       [[[-0.27754477,  0.05196274],\n",
      "         [-0.35064217, -0.18138404]],\n",
      "\n",
      "        [[ 0.37457222,  0.17256688],\n",
      "         [ 0.23161933, -0.09052942]],\n",
      "\n",
      "        [[-0.15711783,  0.23378649],\n",
      "         [ 0.02187908,  0.15757294]],\n",
      "\n",
      "        [[-0.14107615,  0.24936987],\n",
      "         [-0.3135348 ,  0.17329565]],\n",
      "\n",
      "        [[ 0.10231268,  0.33031052],\n",
      "         [-0.27306259, -0.16511877]],\n",
      "\n",
      "        [[-0.28169763, -0.02088608],\n",
      "         [-0.13807018,  0.03726554]],\n",
      "\n",
      "        [[ 0.01727092,  0.24375407],\n",
      "         [ 0.17493658, -0.28089106]],\n",
      "\n",
      "        [[-0.25027281, -0.04175678],\n",
      "         [ 0.95485538,  1.13332093]]],\n",
      "\n",
      "\n",
      "       [[[ 0.67888474,  0.23484872],\n",
      "         [-0.23118755, -0.31190112]],\n",
      "\n",
      "        [[ 0.20661646,  0.10350168],\n",
      "         [-0.16367716, -0.22723618]],\n",
      "\n",
      "        [[-0.24032737,  0.13894987],\n",
      "         [-0.13173769,  0.29435065]],\n",
      "\n",
      "        [[ 0.12375284,  0.27845058],\n",
      "         [-0.08515411,  0.21196808]],\n",
      "\n",
      "        [[ 1.14361596,  0.52906185],\n",
      "         [ 0.40205982,  0.0602163 ]],\n",
      "\n",
      "        [[-0.05878142, -0.06291475],\n",
      "         [ 0.16597642, -0.18913299]],\n",
      "\n",
      "        [[ 0.20732857, -0.03356767],\n",
      "         [-0.11640965,  0.13463396]],\n",
      "\n",
      "        [[ 0.00522251, -0.37634295],\n",
      "         [-0.14136089, -0.25177059]]],\n",
      "\n",
      "\n",
      "       [[[ 0.01127267,  0.04499606],\n",
      "         [-0.2889863 , -0.23509631]],\n",
      "\n",
      "        [[ 0.04137418, -0.13483885],\n",
      "         [ 0.36310515,  0.29972965]],\n",
      "\n",
      "        [[-0.14842387, -0.02364267],\n",
      "         [ 0.02725691,  0.00312781]],\n",
      "\n",
      "        [[-0.23552047,  0.44513133],\n",
      "         [ 0.10868676,  0.09990903]],\n",
      "\n",
      "        [[-0.196288  ,  0.2564334 ],\n",
      "         [ 0.11327478, -0.14949262]],\n",
      "\n",
      "        [[-0.15826409, -0.24716291],\n",
      "         [-0.2858144 ,  0.21651445]],\n",
      "\n",
      "        [[-0.15861569, -0.16769123],\n",
      "         [ 0.02276509, -0.03877999]],\n",
      "\n",
      "        [[ 0.29267493,  1.13209844],\n",
      "         [ 0.2939854 ,  0.29642004]]],\n",
      "\n",
      "\n",
      "       [[[-0.09181574,  0.25976667],\n",
      "         [-0.11286747, -0.15068555]],\n",
      "\n",
      "        [[ 0.35578081, -0.09354425],\n",
      "         [ 0.00919322,  0.28505972]],\n",
      "\n",
      "        [[ 0.18761548,  0.05179596],\n",
      "         [ 0.22433817,  0.27829874]],\n",
      "\n",
      "        [[ 0.08506128, -0.16464487],\n",
      "         [-0.21717037, -0.12637463]],\n",
      "\n",
      "        [[ 0.04753079, -0.14796838],\n",
      "         [-0.108958  , -0.18556991]],\n",
      "\n",
      "        [[-0.10359167, -0.15646568],\n",
      "         [-0.14289536, -0.19119269]],\n",
      "\n",
      "        [[ 0.2097282 , -0.04257431],\n",
      "         [-0.2618781 , -0.20263828]],\n",
      "\n",
      "        [[-0.09106395,  0.08305573],\n",
      "         [-0.07564344, -0.27901664]]],\n",
      "\n",
      "\n",
      "       [[[-0.16652299, -0.13072237],\n",
      "         [ 0.17326275, -0.22173223]],\n",
      "\n",
      "        [[-0.05308767,  0.33886418],\n",
      "         [-0.03574728, -0.25257   ]],\n",
      "\n",
      "        [[ 0.24787164, -0.15378995],\n",
      "         [ 0.08511361,  0.20461756]],\n",
      "\n",
      "        [[ 0.21746725, -0.18281382],\n",
      "         [-0.18276635,  0.13140501]],\n",
      "\n",
      "        [[-0.24956462, -0.19660841],\n",
      "         [-0.1581333 , -0.19063444]],\n",
      "\n",
      "        [[ 0.14258571, -0.06192277],\n",
      "         [ 0.09869915, -0.27092344]],\n",
      "\n",
      "        [[ 0.00475049,  0.23734494],\n",
      "         [-0.05400715, -0.11699145]],\n",
      "\n",
      "        [[-0.16858722, -0.0888114 ],\n",
      "         [-0.04617132, -0.16836685]]],\n",
      "\n",
      "\n",
      "       [[[ 0.65802836, -0.13981897],\n",
      "         [ 0.18239303, -0.08590317]],\n",
      "\n",
      "        [[-0.14731532,  0.10813592],\n",
      "         [ 0.18267626, -0.14134981]],\n",
      "\n",
      "        [[-0.22354841, -0.22582614],\n",
      "         [-0.14959359,  0.20743003]],\n",
      "\n",
      "        [[ 0.41989529,  0.32401508],\n",
      "         [ 0.67555827,  0.28770152]],\n",
      "\n",
      "        [[ 0.65892035, -0.12802547],\n",
      "         [ 0.58993405,  0.37601244]],\n",
      "\n",
      "        [[ 0.28217885,  0.30227217],\n",
      "         [ 0.03844603,  0.10132211]],\n",
      "\n",
      "        [[-0.06154764,  0.24145952],\n",
      "         [ 0.08457903, -0.02571333]],\n",
      "\n",
      "        [[ 0.90392643,  0.12745832],\n",
      "         [-0.00149024,  0.19108775]]],\n",
      "\n",
      "\n",
      "       [[[ 0.12244909,  0.59691453],\n",
      "         [-0.06742636,  0.32981995]],\n",
      "\n",
      "        [[-0.11466245,  0.0840219 ],\n",
      "         [ 0.18550216,  0.24855033]],\n",
      "\n",
      "        [[ 0.0464796 ,  0.14445177],\n",
      "         [-0.05846244,  0.07177221]],\n",
      "\n",
      "        [[ 0.28283289,  0.34496346],\n",
      "         [ 0.55269504,  0.29286423]],\n",
      "\n",
      "        [[-0.23340318,  0.75842005],\n",
      "         [ 0.07425322,  0.91349995]],\n",
      "\n",
      "        [[-0.14683524, -0.20331985],\n",
      "         [ 0.1198704 , -0.25652868]],\n",
      "\n",
      "        [[-0.07223208,  0.07362792],\n",
      "         [ 0.11813711, -0.05023267]],\n",
      "\n",
      "        [[ 0.23845565,  0.95135868],\n",
      "         [-0.00454363, -0.43896523]]]], dtype=float32), array([  9.20221210e-05,  -6.71326816e-02,  -9.76890139e-03,\n",
      "        -8.61299634e-02,  -1.43239889e-02,  -1.71569380e-04,\n",
      "        -3.34244907e-01,  -3.26857865e-01], dtype=float32)]\n",
      "{'activation': 'relu', 'trainable': True, 'name': 'activation_83'}\n",
      "[]\n",
      "{'name': 'maxpooling2d_50', 'trainable': True, 'dim_ordering': 'th', 'pool_size': (2, 2), 'strides': (2, 2), 'border_mode': 'valid'}\n",
      "[]\n",
      "{'p': 0.25, 'trainable': True, 'name': 'dropout_77'}\n",
      "[]\n",
      "{'trainable': True, 'name': 'flatten_29'}\n",
      "[]\n",
      "{'W_constraint': None, 'b_constraint': None, 'name': 'dense_56', 'activity_regularizer': None, 'trainable': True, 'init': 'glorot_uniform', 'bias': True, 'activation': 'linear', 'input_dim': None, 'b_regularizer': {'l2': 0.009999999776482582, 'name': 'WeightRegularizer', 'l1': 0.0}, 'W_regularizer': None, 'output_dim': 128}\n",
      "[array([[-0.00292983, -0.00936657,  0.01000546, ...,  0.00121114,\n",
      "         0.00521186, -0.00242588],\n",
      "       [-0.01025279, -0.00967469,  0.01204072, ..., -0.01272217,\n",
      "         0.01518422, -0.00473263],\n",
      "       [-0.01408983,  0.01406726,  0.00906316, ...,  0.0003177 ,\n",
      "         0.00129712,  0.00804223],\n",
      "       ..., \n",
      "       [ 0.00378149, -0.01471153, -0.00494261, ...,  0.01515157,\n",
      "         0.00448382,  0.00453396],\n",
      "       [ 0.0147572 , -0.00795434,  0.00375634, ..., -0.01317063,\n",
      "         0.00701817, -0.01165483],\n",
      "       [-0.0048005 ,  0.00666447, -0.00965302, ...,  0.01340482,\n",
      "        -0.01070619,  0.00358687]], dtype=float32), array([-0.00541382,  0.01255276,  0.00091014, -0.00479904,  0.01229252,\n",
      "       -0.00018209, -0.00183951,  0.03450437, -0.00299579, -0.0025109 ,\n",
      "        0.01330292, -0.00548751,  0.01297663, -0.00653144, -0.0159534 ,\n",
      "       -0.00264868,  0.00174465, -0.00421471,  0.02306123,  0.00128259,\n",
      "       -0.00262451, -0.00251426, -0.00424946, -0.00159868,  0.01460823,\n",
      "       -0.00050956, -0.00237015, -0.00143593, -0.00122878, -0.00286329,\n",
      "       -0.00404649, -0.00230126,  0.00671483,  0.02132951,  0.0135503 ,\n",
      "        0.00067932,  0.00736036,  0.0017555 , -0.00310863, -0.00471196,\n",
      "        0.02102948, -0.00104021,  0.00986853, -0.00221421, -0.00255841,\n",
      "        0.00531949, -0.0014344 ,  0.00075439,  0.00523761, -0.00227405,\n",
      "        0.00628018, -0.00107336, -0.0034207 , -0.00468286,  0.00876714,\n",
      "       -0.00249356, -0.00303084, -0.00155492,  0.02287633, -0.00298995,\n",
      "       -0.00372566,  0.00997868, -0.00394421,  0.01688448, -0.00242147,\n",
      "       -0.00254138,  0.01618484, -0.00252565, -0.00132726, -0.00154594,\n",
      "        0.01698235,  0.00326256,  0.01765504, -0.00349408, -0.00280666,\n",
      "       -0.00415875,  0.00771269, -0.00097968, -0.00187431, -0.00234275,\n",
      "       -0.0006354 ,  0.01931751, -0.00164962, -0.00324128, -0.00540557,\n",
      "        0.02433834,  0.00148974, -0.00640903,  0.01265515, -0.00261502,\n",
      "       -0.00410477, -0.00224046,  0.00067016, -0.00208428,  0.00305736,\n",
      "       -0.0023652 , -0.00382964,  0.00548803, -0.00158558, -0.00214971,\n",
      "       -0.00201838, -0.00291818,  0.0187454 , -0.00209754, -0.00325431,\n",
      "        0.0347998 ,  0.00898679, -0.00287452,  0.00605662, -0.0023006 ,\n",
      "       -0.00130001,  0.03149083,  0.01194512, -0.00493878,  0.00013665,\n",
      "        0.01706706, -0.00533029, -0.00019385, -0.00590994, -0.00434359,\n",
      "       -0.00193535,  0.00111683, -0.00434053, -0.00183981,  0.02316091,\n",
      "        0.00383121,  0.02939957, -0.00252992], dtype=float32)]\n",
      "{'activation': 'relu', 'trainable': True, 'name': 'activation_84'}\n",
      "[]\n",
      "{'p': 0.5, 'trainable': True, 'name': 'dropout_78'}\n",
      "[]\n",
      "{'W_constraint': None, 'b_constraint': None, 'name': 'dense_57', 'activity_regularizer': None, 'trainable': True, 'init': 'glorot_uniform', 'bias': True, 'activation': 'linear', 'input_dim': None, 'b_regularizer': {'l2': 0.009999999776482582, 'name': 'WeightRegularizer', 'l1': 0.0}, 'W_regularizer': None, 'output_dim': 10}\n",
      "[array([[-0.18782026,  0.04076605, -0.10251498, ..., -0.01529027,\n",
      "         0.18684936, -0.12876341],\n",
      "       [ 0.22510697, -0.20997368,  0.12263617, ..., -0.33976236,\n",
      "        -0.14215317,  0.20918922],\n",
      "       [ 0.19064161,  0.29541031,  0.00845889, ...,  0.00868877,\n",
      "        -0.17672309, -0.22165464],\n",
      "       ..., \n",
      "       [-0.19384828,  0.32114455,  0.02428943, ..., -0.01512013,\n",
      "        -0.10477838, -0.28187564],\n",
      "       [-0.18184319, -0.22290492, -0.18064557, ...,  0.4142876 ,\n",
      "         0.08438111,  0.15716587],\n",
      "       [-0.01818611,  0.02400777, -0.14073363, ..., -0.18856047,\n",
      "         0.04923525,  0.21085396]], dtype=float32), array([  8.52174871e-03,  -1.33787107e-05,  -9.50448029e-03,\n",
      "         1.99426152e-03,   2.06745835e-03,  -2.44516740e-03,\n",
      "         1.34393643e-03,   3.58568202e-03,  -5.37845865e-03,\n",
      "        -1.71605614e-04], dtype=float32)]\n",
      "{'activation': 'softmax', 'trainable': True, 'name': 'activation_85'}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print layer.get_config()\n",
    "    print layer.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('./model/saved_weights_converged.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Visualise what the CNN has learnt\n",
    "Inspired by https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py and https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'convolution2d_39 '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-f3c6f9e3b7eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlayer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'convolution2d_39 '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlayer_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Number of filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'convolution2d_39 '"
     ]
    }
   ],
   "source": [
    "# the name of the layer we want to visualize\n",
    "# (see model definition at keras/applications/vgg16.py)\n",
    "layer_name = 'convolution2d_39 '\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "layer_output = layer_dict[layer_name].output\n",
    "\n",
    "# Number of filters\n",
    "n = 8\n",
    "\n",
    "# the filters that have the highest loss are assumed to be better-looking.\n",
    "# we will only keep the top 64 filters.\n",
    "kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
    "                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
    "\n",
    "# save the result to disk\n",
    "imsave('stitched_filters_%dx%d.png' % (n, n), stitched_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict based on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "def load_test_images(test_images_dir):\n",
    "    total = 0\n",
    "    images = []\n",
    "    image_ids = []\n",
    "    test_image_list = os.listdir(test_images_dir)\n",
    "    num_test_images = len(test_image_list)\n",
    "    for i in range(0,num_test_images):\n",
    "        filename = test_images_dir + test_image_list[i]\n",
    "        image = cv2.imread(filename, 0).transpose()  # Is the color_type needed here as these are pre-processed images??\n",
    "        images.append(image)  \n",
    "        image_ids.append(test_image_list[i])\n",
    "        total += 1\n",
    "        if total % 100 == 0:\n",
    "            print \".\",\n",
    "    print \"\\nProcesses {} rows.\".format(total)\n",
    "    \n",
    "    images = np.array(images, dtype=np.uint8)\n",
    "    images = images.reshape(images.shape[0], color_type, image_width, image_height)\n",
    "    images = images.astype('float32')\n",
    "    images /= 255\n",
    "    \n",
    "    return images, np.array(image_ids)\n",
    "\n",
    "#test_image_batches = split_test_data(test_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "Processes 79726 rows.\n",
      "(79726, 1, 80, 60)\n",
      "(79726,)\n"
     ]
    }
   ],
   "source": [
    "test_data, test_ids = load_test_images(test_images_dir)  \n",
    "print test_data.shape\n",
    "print test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79726/79726 [==============================] - 55s    \n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_data, batch_size = 128, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79726, 10)\n"
     ]
    }
   ],
   "source": [
    "print predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def create_submission(predictions, test_ids, test_info):\n",
    "    result = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result.loc[:, 'img'] = pd.Series(test_ids, index=result.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('submission'):\n",
    "        os.mkdir('submission')\n",
    "    suffix = test_info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('submission', 'submission_' + suffix + '.csv')\n",
    "    result.to_csv(sub_file, index=False)\n",
    "\n",
    "test_info = 'loss_' + str(score) \\\n",
    "                + '_h_' + str(image_height) \\\n",
    "                + '_w_' + str(image_width) \\\n",
    "                + '_ep_' + str(num_epochs)\n",
    "create_submission(predictions, test_ids, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###VGG16 model\n",
    "From https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3\n",
    "Also see:\n",
    "http://blog.christianperone.com/2016/01/convolutional-hypercolumns-in-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg16(num_classes, weights_path=None):\n",
    "    \n",
    "    # Create callback for history report\n",
    "    from keras.callbacks import Callback\n",
    "    class LossHistory(Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        print \"Loading weights from {}\".format(weights_path)\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "    # Now replace the top layer with one for our own purposes\n",
    "    model.layers.pop()\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    #TODO - Rework model based on \n",
    "    return model, LossHistory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
