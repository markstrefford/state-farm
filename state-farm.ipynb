{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the State Farm image data\n",
    "\n",
    "This notebook provides analysis of the provided State Farm data, using Theano and Keras to build the NN.\n",
    "\n",
    "Note that the data is available from Kaggle here:  \n",
    "https://www.kaggle.com/c/state-farm-distracted-driver-detection/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "First, let's import what we need and set up environment variables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports of the relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# These are the locations of the images provided by Kaggle\n",
    "# Root Dir is needed for Python, but not for create lmdb shell script later... (we need it there too!)\n",
    "image_root_dir = './imgs/'\n",
    "train_image_source_dir = \"./train/\"\n",
    "test_image_source_dir = \"./test/\"\n",
    "driver_image_list = \"./driver_imgs_list.csv\"\n",
    "\n",
    "# These are the locations of the images that we will work with \n",
    "# Note that as we're continually mix up training and validation drivers/images, \n",
    "# then we will store images in one directory and use code to determine whether to train or validate\n",
    "train_images_dir = \"./images/train/\"\n",
    "#validation_images_dir = \"./images/validate/\" \n",
    "test_images_dir = \"./images/test/\"\n",
    "\n",
    "# Some more controls\n",
    "# color type: 1 - grey, 3 - rgb\n",
    "color_type = 1\n",
    "image_width = 80\n",
    "image_height = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by pre-processing the images\n",
    "There are only 27 different drivers so in order to avoid overfitting, or testing using very similar data to training, we will split the data based on the driver into train and validation sets.\n",
    "\n",
    "Initially though, let's get the list of drivers, see how many images are available for each driver, and which classification they have been labelled with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data summary: \n",
      "  subject classname            img\n",
      "0    p002        c0  img_44733.jpg\n",
      "1    p002        c0  img_72999.jpg\n",
      "2    p002        c0  img_25094.jpg\n",
      "3    p002        c0  img_69092.jpg\n",
      "4    p002        c0  img_92629.jpg\n",
      "\n",
      "Testing data summary: \n",
      "['img_1.jpg', 'img_10.jpg', 'img_100.jpg', 'img_1000.jpg', 'img_100000.jpg', 'img_100001.jpg', 'img_100002.jpg', 'img_100003.jpg', 'img_100004.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Training set is in the provided csv file\n",
    "driver_list = pd.read_csv(driver_image_list)\n",
    "print \"Training data summary: \\n{}\".format(driver_list.head())\n",
    "\n",
    "test_image_list = os.listdir(image_root_dir + test_image_source_dir)\n",
    "print \"\\nTesting data summary: \\n{}\".format(test_list[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process images so that they are in an format more suited to training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images found 22424\n"
     ]
    }
   ],
   "source": [
    "def get_driver_images_and_classes(driver_list):\n",
    "    image_list = []\n",
    "    class_list = []\n",
    "    total = 0\n",
    "    for driver_row in [ drvr for drvr in driver_list.iterrows() ]:   # if drvr[1]['subject'] in filter \n",
    "        driver = driver_row[1]  # Drop the index created by the Pandas Dataframe\n",
    "        driver_class = int(driver['classname'][1:])  # Get integer to represent class (eg 'c0' is class '0')\n",
    "        image_list.append(driver['img'])\n",
    "        class_list.append(driver_class)\n",
    "        total += 1\n",
    "    print \"Total number of training images found {}\".format(total)\n",
    "    #Return a list of images and their classification\n",
    "    return np.array(image_list), np.array(class_list)\n",
    "\n",
    "# Create a training list of images and classes from the training set\n",
    "images, classes = get_driver_images_and_classes(driver_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Process the image, for now this is resize only\n",
    "# We'll handle colour/greyscale when we load as cv2 does this for us\n",
    "# TODO - Move to directory creaton to Python code to be OS independent\n",
    "\n",
    "def pre_process_image(image):\n",
    "    processed_img = cv2.resize(image, (image_width, image_height)) \n",
    "    return processed_img\n",
    "    \n",
    "def create_train_image_repository(images_dest_dir, images_list, class_list, color_type=1):\n",
    "    print \"create_image_repository(): Processing images into {}\".format(images_dest_dir)\n",
    "    copied = 0 \n",
    "    for f, c in zip(images_list, class_list):\n",
    "        dest_dir = images_dest_dir + str(c) + \"/\"\n",
    "        if os.path.exists(dest_dir) == False:\n",
    "            !mkdir $dest_dir\n",
    "        image_filename = image_root_dir + train_image_source_dir + '/c' + str(c) + '/' + f\n",
    "        if color_type == 1:\n",
    "            orig_img = cv2.imread(image_filename, 0)\n",
    "        elif color_type == 3:\n",
    "            orig_img = cv2.imread(image_filename)\n",
    "        processed_image = pre_process_image(orig_img)\n",
    "        cv2.imwrite(images_dest_dir + str(c) + \"/\" + f, processed_image)\n",
    "        copied += 1\n",
    "        if copied % 100 == 0:\n",
    "            print \".\",\n",
    "    print \"\\nCopied {} images...Done!\".format(copied)\n",
    "    return copied\n",
    "\n",
    "def create_test_image_repository(dest_dir, images_list, color_type=1):\n",
    "    print \"create_image_repository(): Processing images into {}\".format(dest_dir)\n",
    "    copied = 0 \n",
    "    for f in images_list:\n",
    "        if os.path.exists(dest_dir) == False:\n",
    "            !mkdir $dest_dir\n",
    "        image_filename = image_root_dir + test_image_source_dir + f\n",
    "        if color_type == 1:\n",
    "            orig_img = cv2.imread(image_filename, 0)\n",
    "        elif color_type == 3:\n",
    "            orig_img = cv2.imread(image_filename)\n",
    "        processed_image = pre_process_image(orig_img)\n",
    "        cv2.imwrite(dest_dir + f, processed_image)\n",
    "        copied += 1\n",
    "        if copied % 100 == 0:\n",
    "            print \".\",\n",
    "    print \"\\nCopied {} images...Done!\".format(copied)\n",
    "    return copied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process images if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_image_repository(): Processing images into ./images/test/\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "Copied 79726 images...Done!\n",
      "create_image_repository(): Processing images into ./images/train/\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "Copied 22424 images...Done!\n"
     ]
    }
   ],
   "source": [
    "# Start by clearing out any old data (ignore failures here if the directory doesn't exist)\n",
    "# TODO - Move to Python code to be OS independent\n",
    "\n",
    "create_repository = True    # True forces creation of the processed images, \n",
    "                            # Set to False if this has been done previously\n",
    "if create_repository:\n",
    "    !rm -rf $train_images_dir\n",
    "    #!rm -rf $validation_images_dir\n",
    "    !rm -rf $test_images_dir\n",
    "\n",
    "    # Create directories\n",
    "    !mkdir -p $train_images_dir\n",
    "    #!mkdir -p $validation_images_dir\n",
    "    !mkdir -p $test_images_dir\n",
    "\n",
    "    create_test_image_repository(test_images_dir, test_image_list, color_type=color_type)\n",
    "    create_train_image_repository(train_images_dir, images, classes, color_type=color_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and validation data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the drivers into a training and validation set.  To ensure we don't have overfitting (the training set and the validation set contain the same or similar images) we will split on drivers, so a driver can only appear in training or validation but not both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following drivers: ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081']\n"
     ]
    }
   ],
   "source": [
    "driver_ids = []\n",
    "for id, driver in driver_list.iterrows():\n",
    "    if driver['subject'] not in driver_ids:\n",
    "        driver_ids.append(driver['subject'])\n",
    "print \"Found the following drivers: {}\".format(driver_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and validation data tests (split = percentage to have in training set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver train list: ['p050' 'p045' 'p042' 'p012' 'p064' 'p041' 'p015' 'p024' 'p022' 'p066'\n",
      " 'p072' 'p052' 'p039' 'p061' 'p081' 'p014' 'p035' 'p002' 'p056' 'p016'\n",
      " 'p026' 'p021' 'p075' 'p049']\n",
      "Driver validation list: ['p047', 'p051']\n"
     ]
    }
   ],
   "source": [
    "def split_drivers_into_train_and_validate(driver_list, split = 0.95):\n",
    "    driver_valid_list = []\n",
    "    # Take a random sample of drivers into the training list\n",
    "    driver_train_list = np.random.choice(driver_list, int(len(driver_list)*split), replace = False)\n",
    "    # Take the remaining drivers into the validation list\n",
    "    driver_valid_list = [ driver for driver in driver_list if driver not in driver_train_list]\n",
    "    return driver_train_list, driver_valid_list\n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_all.ix[rows], student_data[target_col].ix[rows], test_size=test_size)\n",
    "\n",
    "training_list, validation_list = split_drivers_into_train_and_validate(driver_ids)\n",
    "print \"Driver train list: {}\".format(training_list)\n",
    "print \"Driver validation list: {}\".format(validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def render_image(image_filename):\n",
    "    print \"render_image(): Rendering {}\".format(image_filename)\n",
    "    image = cv2.imread(image_filename, color_type_global)\n",
    "    plt.axis(\"off\")\n",
    "    #plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.imshow(image)\n",
    "    plt.show() \n",
    "    #print image.shape\n",
    "    #plt.imshow(image)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "Processes 20669 rows.\n",
      "Creating validation data:\n",
      ". . . . . . . . . . . . . . . . . \n",
      "Processes 1755 rows.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "def create_train_validation_data(driver_list, filter):\n",
    "    #sample = driver_list[driver_list.subject.isin(filter)].ix[:, 'classname':'img']\n",
    "    images = []\n",
    "    labels = []\n",
    "    total = 0\n",
    "    for driver_row in [ drvr for drvr in driver_list[driver_list.subject.isin(filter)].ix[:, 'classname':'img'].iterrows() ]:   # if drvr[1]['subject'] in filter \n",
    "        driver = driver_row[1]  # Drop the index created by the Pandas Dataframe\n",
    "        #print driver\n",
    "        label = int(driver['classname'][1:])\n",
    "        filename = train_images_dir + str(label) + \"/\" + driver['img']\n",
    "        #print filename\n",
    "        image = cv2.imread(filename, 0).transpose()  # Is the color_type needed here as these are pre-processed images??\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "        total += 1\n",
    "        if total % 100 == 0:\n",
    "            print \".\",\n",
    "    print \"\\nProcesses {} rows.\".format(total)\n",
    "    \n",
    "    images = np.array(images, dtype=np.uint8)\n",
    "    images = images.reshape(images.shape[0], color_type, image_width, image_height)\n",
    "    images = images.astype('float32')\n",
    "    images /= 255\n",
    "    \n",
    "    labels = np.array(labels, dtype=np.uint8)\n",
    "    labels = np_utils.to_categorical(labels, 10)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# TODO - Add in random ordering of training data!!\n",
    "# index = np.random.choice(range(0, num_training_samples), num_training_samples, replace = False) # Random ordering\n",
    "# ...driver_list[index], training_list[index]\n",
    "print \"Creating training data:\"\n",
    "X_train, y_train = create_train_validation_data(driver_list, training_list)\n",
    "print \"Creating validation data:\"\n",
    "X_valid, y_valid = create_train_validation_data(driver_list, validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20669, 1, 80, 60)\n",
      "(20669, 10)\n",
      "(1755, 1, 80, 60)\n",
      "(1755, 10)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "\n",
    "print X_valid.shape\n",
    "print y_valid.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inital CNN using Keras\n",
    "Starting with no pre-loaded weights though as we'll train this with our own data.\n",
    "Based on example here http://keras.io\n",
    "\n",
    "TODO: In a future iteration, we'll play about with this architecture and the activation, optimizer and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D  \n",
    "from keras.layers import Dense, Flatten, Activation, Dropout, MaxPooling1D\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Custom Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def custom_keras_model(num_classes, weights_path=None):\n",
    "    num_filters = 8      #number of filters to apply/learn in the 1D convolutional layer\n",
    "    num_pooling = 2\n",
    "    filter_length = 2     #linear length of each filter (this is 1D)\n",
    "\n",
    "    # Create callback for history report\n",
    "    from keras.callbacks import Callback\n",
    "    class LossHistory(Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "    \n",
    "    \n",
    "    #from keras.utils.dot_utils import Grapher\n",
    "    \n",
    "    model = Sequential()\n",
    "    #grapher = Grapher()\n",
    "\n",
    "    # Now create the NN architecture (version 1)\n",
    "    # Going with colour for now!!\n",
    "    model.add(Convolution2D(num_filters, filter_length, filter_length, border_mode=\"valid\", \n",
    "                        activation=\"relu\", \n",
    "                        input_shape=(color_type, image_width, image_height)))\n",
    "\n",
    "    model.add(Convolution2D(num_filters, filter_length, filter_length))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(num_pooling, num_pooling)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    if weights_path:\n",
    "        print \"Loading weights from {}\".format(weights_path)\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "\n",
    "    #model.summary()\n",
    "    #grapher.plot(model, 'nn_model.png')\n",
    "    \n",
    "    # TODO - Handle loading existing weights \n",
    "    \n",
    "    return model, LossHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###VGG16 model\n",
    "From https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3\n",
    "Also see:\n",
    "http://blog.christianperone.com/2016/01/convolutional-hypercolumns-in-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg16(num_classes, weights_path=None):\n",
    "    \n",
    "    # Create callback for history report\n",
    "    from keras.callbacks import Callback\n",
    "    class LossHistory(Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        print \"Loading weights from {}\".format(weights_path)\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "    # Now replace the top layer with one for our own purposes\n",
    "    #model.layers.pop()\n",
    "    #model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    #TODO - Rework model based on \n",
    "    return model, LossHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine which NN we are going to use, and whether to load weights or train ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#keras_model, weights, train_model = 'custom', None, True\n",
    "keras_model, weights, train_model = 'vgg16', 'model/vgg16_weights.h5', False\n",
    "loss_function='categorical_crossentropy'\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights from model/vgg16_weights.h5 ?\n",
      "Loading weights from model/vgg16_weights.h5\n"
     ]
    }
   ],
   "source": [
    "#Configure the network\n",
    "\n",
    "if keras_model == 'custom':\n",
    "    model, LossHistory = custom_keras_model(num_classes, weights)\n",
    "    sgd = SGD(lr=0.1, decay=0, momentum=0, nesterov=False)\n",
    "elif keras_model == 'vgg16':\n",
    "    model, LossHistory = vgg16(num_classes, weights)\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Now compile the model\n",
    "model.compile(loss=loss_function, optimizer=sgd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "Train and use validation data to see if we're training effectively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20669 samples, validate on 1755 samples\n",
      "Epoch 1/5\n",
      "50s - loss: 1.2939 - val_loss: 1.4075\n",
      "Epoch 2/5\n",
      "48s - loss: 0.3134 - val_loss: 1.9248\n",
      "Epoch 3/5\n",
      "48s - loss: 0.1751 - val_loss: 2.0871\n",
      "Epoch 4/5\n",
      "48s - loss: 0.1301 - val_loss: 2.4730\n",
      "Epoch 5/5\n",
      "48s - loss: 0.1068 - val_loss: 2.2622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1228fec10>"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "history = LossHistory()\n",
    "index = np.random.choice(range(0, num_training_samples), num_training_samples, replace = False) # Random ordering\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=num_epochs,\n",
    "              show_accuracy=True, verbose=2, validation_data=(X_valid, y_valid), \n",
    "              callbacks=[history])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAADhCAYAAADWOvI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ4OWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1229df250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def graph_training_loss_history(losses):\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(losses)\n",
    "    plt.ylabel('error')\n",
    "    plt.xlabel('batch')\n",
    "    plt.title('training error')\n",
    "    plt.show()\n",
    "    \n",
    "graph_training_loss_history(history.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO : Add in model.load_weights(filename) here at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1755/1755 [==============================] - 1s     \n",
      "('Score log_loss: ', 2.2638033781433351)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "predictions_valid = model.predict(X_valid, batch_size=128, verbose=1)\n",
    "score = log_loss(y_valid, predictions_valid)\n",
    "print('Score log_loss: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('./model/saved_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict based on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "def load_test_images(test_images_dir):\n",
    "    total = 0\n",
    "    images = []\n",
    "    image_ids = []\n",
    "    test_image_list = os.listdir(test_images_dir)\n",
    "    num_test_images = len(test_image_list)\n",
    "    for i in range(0,num_test_images):\n",
    "        filename = test_images_dir + test_image_list[i]\n",
    "        image = cv2.imread(filename, 0).transpose()  # Is the color_type needed here as these are pre-processed images??\n",
    "        images.append(image)  \n",
    "        image_ids.append(test_image_list[i])\n",
    "        total += 1\n",
    "        if total % 100 == 0:\n",
    "            print \".\",\n",
    "    print \"\\nProcesses {} rows.\".format(total)\n",
    "    \n",
    "    images = np.array(images, dtype=np.uint8)\n",
    "    images = images.reshape(images.shape[0], color_type, image_width, image_height)\n",
    "    images = images.astype('float32')\n",
    "    images /= 255\n",
    "    \n",
    "    return images, np.array(image_ids)\n",
    "\n",
    "#test_image_batches = split_test_data(test_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "Processes 79726 rows.\n",
      "(79726, 1, 80, 60)\n",
      "(79726,)\n"
     ]
    }
   ],
   "source": [
    "test_data, test_ids = load_test_images(test_images_dir)  \n",
    "print test_data.shape\n",
    "print test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79726/79726 [==============================] - 55s    \n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_data, batch_size = 128, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79726, 10)\n"
     ]
    }
   ],
   "source": [
    "print predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def create_submission(predictions, test_ids, test_info):\n",
    "    result = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result.loc[:, 'img'] = pd.Series(test_ids, index=result.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('submission'):\n",
    "        os.mkdir('submission')\n",
    "    suffix = test_info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('submission', 'submission_' + suffix + '.csv')\n",
    "    result.to_csv(sub_file, index=False)\n",
    "\n",
    "test_info = 'loss_' + str(score) \\\n",
    "                + '_h_' + str(image_height) \\\n",
    "                + '_w_' + str(image_width) \\\n",
    "                + '_ep_' + str(num_epochs)\n",
    "create_submission(predictions, test_ids, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
